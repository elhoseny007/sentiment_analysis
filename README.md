sentiment analysis with Deep_learning
ğŸ“Œ Project Overview

This repository contains an end-to-end sentiment analysis pipeline trained on the Sentiment140 dataset (1.6M Tweets).
The model predicts whether a tweet is:

Negative (0)

Neutral (2)

Positive (4)

The implementation covers data preprocessing, visualization, deep learning with GRU/LSTM, and evaluation.

âš™ï¸ Key Features

âœ… Language detection: Automatically detect tweet language using langdetect.

âœ… Text cleaning: Remove punctuation, special characters, and stopwords for multiple languages.

âœ… Normalization: Apply lemmatization to unify words (e.g., running â†’ run).

âœ… Visualization:

Top 50 frequent words

WordCloud of most common terms

âœ… Deep Learning model: Bidirectional GRU stacked layers with dropout for generalization.

âœ… Training strategies:

EarlyStopping (stop training when val_accuracy doesnâ€™t improve)

ReduceLROnPlateau (reduce learning rate on stagnation)

ModelCheckpoint (save best model automatically)

âœ… Evaluation: Accuracy, classification report, confusion matrix.

ğŸ“‚ Dataset

ğŸ“Œ Sentiment140 dataset â€“ 1.6M labeled tweets.

Dataset file:

training.1600000.processed.noemoticon.csv


Columns after renaming in preprocessing:

Target â†’ Sentiment label (0 = Negative, 2 = Neutral, 4 = Positive)

Ids â†’ Tweet ID

Date â†’ Timestamp of tweet

Flag â†’ Query info (unused)

User â†’ Username of the tweet

Text â†’ Actual tweet

ğŸ› ï¸ Installation

Clone repository:

git clone https://github.com/your-username/sentiment-analysis.git
cd sentiment-analysis


Install dependencies:

pip install -r requirements.txt


Or install core libraries:

pip install langdetect gensim wordcloud spacy plotly seaborn nltk tensorflow keras


Download NLTK resources before running:

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

ğŸ”„ Preprocessing Pipeline

Steps performed in code:

Remove Punctuation

tweet = re.sub(r'[^\w\s]', '', tweet)


Language Detection + Stopword Removal

lang = detect(tweet)
if lang in all_stopwords:
    tweet = " ".join([w for w in tweet.split() if w.lower() not in all_stopwords[lang]])


Lemmatization (normalize words)

lemmatizer = WordNetLemmatizer()
df['Clean_Text'] = df['Clean_Text'].apply(lambda t: " ".join([lemmatizer.lemmatize(w) for w in t.split()]))


Tokenization & Padding

Use Tokenizer and pad_sequences to prepare inputs.

Maximum length: 50 tokens per tweet.

ğŸ§  Model Architecture

Built using Keras Functional API:

Embedding Layer â†’ Learn dense representations of words

Bidirectional GRU Layer (256 units) â†’ Context from both directions

Dropout (0.3) â†’ Prevent overfitting

Bidirectional GRU Layer (128 units)

Dense Layer (256 units, ReLU)

Output Layer (Softmax) â†’ 3 classes (Negative, Neutral, Positive)

Loss: categorical_crossentropy
Optimizer: Adam

ğŸš€ Training
history = model.fit(
    X_train, y_train,
    batch_size=1024,
    epochs=5,
    validation_split=0.1,
    callbacks=callbacks
)


Callbacks used:

ModelCheckpoint â†’ Save best .keras model

ReduceLROnPlateau â†’ Lower LR on plateau

EarlyStopping â†’ Stop if no val_accuracy improvement

ğŸ“Š Evaluation
predictions = model.predict(X_test)
print("Test Accuracy:", accuracy_score(true, predicted))
print(classification_report(true, predicted))


Accuracy achieved: ~81%

Outputs a full classification report with precision, recall, F1-score

ğŸ“ˆ Visualizations

Top 50 most frequent words in dataset

WordCloud for most common terms

Training curves:

Loss vs Epochs

Accuracy vs Epochs

ğŸ’¾ Saving

Cleaned data saved to:

/content/drive/MyDrive/sentiment_analysis/data_cleaned.csv


Trained model saved to:

/content/drive/MyDrive/sentiment_analysis/sentiment_model.keras

ğŸ”® Future Work

Add Transformer-based models (BERT, RoBERTa, DistilBERT)

Hyperparameter tuning with larger embedding sizes

Deploy as a Flask / FastAPI web app

Integrate real-time sentiment prediction with Twitter API

ğŸ“ License

This project is licensed under the MIT License.
